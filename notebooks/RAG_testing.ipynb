{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out creating a Vector database with Chroma and Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"../data/rag_txt/\"\n",
    "\n",
    "# List all files and directories in the specified folder\n",
    "files_and_directories = os.listdir(folder_path)\n",
    "\n",
    "# Filter out only the file paths\n",
    "file_paths = [os.path.join(folder_path, file) for file in files_and_directories if os.path.isfile(os.path.join(folder_path, file))]\n",
    "\n",
    "# Print the file paths\n",
    "for file_path in file_paths:\n",
    "    print(file_path)\n",
    "\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "doc_list = []\n",
    "for doc in file_paths:\n",
    "    raw_documents = TextLoader(doc).load()\n",
    "    #text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=20)\n",
    "    #documents = text_splitter.split_documents(raw_documents)\n",
    "    doc_list.append(raw_documents[0])\n",
    "\n",
    "print(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.llms import (\n",
    "    HuggingFaceHub,\n",
    ")\n",
    "from langchain.embeddings import HuggingFaceEmbeddings \n",
    "import chromadb\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_list,\n",
    "    embedding=HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    ),\n",
    "    persist_directory=\"./chromadb_4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tuned team used this prompt\n",
    "prompt_template = \"\"\"\n",
    "### query: {query}\n",
    "\n",
    "### Context: {context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt = PromptTemplate.from_template(\n",
    "    prompt_template\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "\n",
    "chain = (\n",
    "        {\n",
    "        \"context\": itemgetter(\"query\") \n",
    "            | retriever, \n",
    "            #| RunnableLambda(format_docs),\n",
    "        \"query\": itemgetter(\"query\"),\n",
    "    }\n",
    "    | prompt\n",
    "   # | ft_llm\n",
    "   # | StrOutputParser()\n",
    ")\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What do poeple think about crime\"\n",
    "rag_prompt = chain.invoke({\"query\": query}).to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#porject specfic\n",
    "import boto3\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "import pandas as pd\n",
    "#local\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "from pathlib import Path\n",
    "\n",
    "final_prompt_template = PromptTemplate(\n",
    "    # template = \"\"\"\n",
    "    # \\n\\nHuman: You are a super helpful robot that does exactly as told.\n",
    "    # Outline the purpose of this poll and briefly describe the data.\n",
    "    # Include where the poll came from, the question number, and a description of the CSV file.\n",
    "    # Do not exceed more than 200 words.\n",
    "    # write the poll name, the table number, the question and a summary of the data\n",
    "    # {data}\n",
    "    # Do not repeat instructions back to me, just complete the task.\n",
    "    #  \\n\\nAssistant:\"\"\",\n",
    "    template = \"\"\"\n",
    "    \\n\\nHuman: You are a helpful administrative assistant.\n",
    "Using only the context provided, detail which questions from polls may be relevant to this query.\n",
    "For each question, return the file name, question and question text. Give a short summary of the findings.\n",
    "Here is the query and context:\n",
    "{rag}\n",
    "Do not use any information not in the context.\n",
    "If there is no relevant information say 'I dont know'\n",
    "\\n\\nAssistant:\"\"\",\n",
    "     input_variables=[\"rag\"])\n",
    "     \n",
    "\n",
    "\n",
    "class LLMHandler:\n",
    "    \"\"\"LLM handler class - set up client\n",
    "    \"\"\"\n",
    "    def __init__(self, env_path):\n",
    "        load_dotenv(env_path)\n",
    "        boto3.setup_default_session(aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "                            aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "                           aws_session_token = os.getenv('AWS_SESSION_TOKEN'))\n",
    "        client = boto3.client(service_name='bedrock-runtime',\n",
    "                       region_name=os.getenv('AWS_DEFAULT_REGION'))\n",
    "        self.llm = BedrockChat(model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "              client = client,\n",
    "             )\n",
    "        self.problem_cases = {}\n",
    "    def get_response(self,\n",
    "                     prompt,\n",
    "                     rag_prompt):\n",
    "        \"\"\"summarise input speech data\n",
    "        Args:\n",
    "            prompt (langchain PromptTemplate): prompt to be passed to LLM\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        chain = prompt | self.llm\n",
    "        response = chain.invoke({\"prompt\":prompt,\"rag\":rag_prompt})\n",
    "        return response\n",
    "#%%\n",
    "#---- prompt\n",
    "\n",
    "## --------------- set up CFG adn LLM\n",
    "# base_dir = Path(os.getcwd()).parents[0]\n",
    "# #data_dir = base_dir /'data'\n",
    "# #example_data_path = data_dir / 'savanta_data'\n",
    "# #poll_files = [x for x in os.listdir(example_data_path) if '.csv' in x]\n",
    "# #%%\n",
    "# #handler = LLMHandler(env_path =base_dir/ '.env')\n",
    "# for file in poll_files[0:1]:\n",
    "#     data = pd.read_csv(example_data_path /file)\n",
    "#     response = handler.get_data(prompt=prompt,\n",
    "#                         data = data.to_csv())\n",
    "#%%\n",
    "# with open(f\"{file}.txt\", \"w\") as file:\n",
    "#     file.write(response.dict()['content'])\n",
    "# data_path = data_dir/'savanta_data/Omni_W184_HomelessAndPolicePR_tables_Private.xlsx'\n",
    "# sheets = parse_santava_excel(data_path)\n",
    "# poll_data = ''\n",
    "# for s in sheets:\n",
    "#     poll_data = ''.join([poll_data, s.to_csv()])\n",
    "# response = handler.get_data(prompt=prompt,\n",
    "#                         data = poll_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(os.getcwd()).parents[0]\n",
    "handler = LLMHandler(env_path =base_dir/ '.env')\n",
    "response = handler.get_response(prompt = final_prompt_template, rag_prompt = rag_prompt)\n",
    "print(response.dict()['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
